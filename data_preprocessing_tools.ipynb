{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np # scientific computing library - perform fast operations on arrays\n",
        "import matplotlib.pyplot as plt # graphs library\n",
        "import pandas as pd # data analysis library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Country   Age   Salary Purchased\n",
            "0   France  44.0  72000.0        No\n",
            "1    Spain  27.0  48000.0       Yes\n",
            "2  Germany  30.0  54000.0        No\n",
            "3    Spain  38.0  61000.0        No\n",
            "4  Germany  40.0      NaN       Yes\n",
            "5   France  35.0  58000.0       Yes\n",
            "6    Spain   NaN  52000.0        No\n",
            "7   France  48.0  79000.0       Yes\n",
            "8  Germany  50.0  83000.0        No\n",
            "9   France  37.0  67000.0       Yes\n"
          ]
        }
      ],
      "source": [
        "# Retrieve data frame from pandas\n",
        "dataframe = pd.read_csv('Data.csv')\n",
        "print(dataframe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### About Dependent Variables\n",
        "The dependent variable is what you want to predict. In the above dataset, the dependent variable is the purchase column -- the other columns are features. The dependent variable is usually the last column in the dataset.\n",
        "\n",
        "We always separate our dependent variable from the features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate the dependent variable from the matrix of features.\n",
        "\n",
        "# The matrix of features.\n",
        "# \":\" means a range. No lower or upper bound means select all columns.\n",
        "# Retrieve all columns except the last column, then return only values.\n",
        "x = dataframe.iloc[:, :-1].values\n",
        "\n",
        "# The dependent variable vector.\n",
        "y = dataframe.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Taking care of missing data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-learn is a machine learning library with data preprocessing tools.\n",
        "\n",
        "We can replace empty values with an average value from the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reveal 0 values in dataset\n",
        "# print((dataframe == 0).sum)\n",
        "\n",
        "# Replace empty values with an average value\n",
        "from sklearn.impute import SimpleImputer\n",
        "# Call the SimpleImputer Class\n",
        "# Specify the missing values you want to replace (np.nan) \"not a number\"\n",
        "# Specify the strategy (mean)\n",
        "imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
        "# fit() will connect the imputer and the matrix of features.\n",
        "# transform() will execute the change.\n",
        "# Select all the numerical columns. \n",
        "# Start at col1 - ending at col3 not including col3.\n",
        "imputer.fit(x[:, 1:3])\n",
        " # Transform | Start at col1 - ending at col3 not including col3.\n",
        "# returns a new matrix with replacement values\n",
        "x[:, 1:3] = imputer.transform(x[:, 1:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data\n",
        "\n",
        "For the machine learning algorithm to interpret correlations between the features and the dependent variable, we must turn the strings into numbers. However, we don't want our machine learning algorithm to interpret that order matters. How do we solve this?\n",
        "\n",
        "### One-hot encoding\n",
        "\n",
        "One-hot encoding is a powerful technique to treat categorical data, but it can lead to increased dimensionality, sparsity, and overfitting. One-hot encoding creates many features. So the country feature becomes 3 columns, one for each country. \n",
        "\n",
        "One-hot encoding creates binary vectors for each or the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# passthrough keeps columns not affected by OneHotEncoding\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
        "# fit and transform at the same time\n",
        "# force output to be a numpy array\n",
        "x = np.array(ct.fit_transform(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Use the label encoder when you have two labels (no,yes) => (0,1)\n",
        "# These labels can be converted into a binary outcome.\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Doesn't need to be a numpy array like what is expected by the feature\n",
        "# machine learning models\n",
        "\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import train test split library\n",
        "from sklearn.model_selection import train_test_split\n",
        "# create 4 variables from return values\n",
        "# x = matrix of features\n",
        "# y = dependent variable\n",
        "# random_state fixes the seed so that consecutive runs split the data the same way (not required, only for education purposes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, train_size=0.8,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature scaling normalizes features to prevent one feature from dominating the other. \n",
        "\n",
        "### Do's and Don'ts\n",
        "\n",
        "---\n",
        "- __Do__ feature scaling after splitting to __prevent information leakage__. You're not suppose to work with the test dataset when training the model. \n",
        "---\n",
        "- __Don't__ apply feature scaling on dummy variables.\n",
        "  - Doing so would destroy the integrity of the data making it harder to interpret. For example, applying standardization would change 0 and 1s to -3 and 3s. It would then be hard to know which feature is active for a given row.\n",
        "\n",
        "- __Don't__ include the test dataset when training the model.\n",
        "  - You are not suppose to use the test dataset when training the model. The test dataset should appear as new data to the model during our tests. \n",
        "  \n",
        "- __Don't__ apply feature scaling before splitting data.\n",
        "  - Feature scaling gets the mean and standard deviation of the feature. If we apply feature scaling before the split it will get the mean and the standard deviation of all the values including the ones in the test set. This would create information leakage. \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# no parameters required\n",
        "sc = StandardScaler()\n",
        "\n",
        "# don't apply standardization to the dummy variables, you will loose the meaning \n",
        "# of the data. Keep the interpretability of the model.\n",
        "\n",
        "# fit get the mean and standard deviation of each of the features.\n",
        "# transform applies the formula to the features so everything ends up at the same scale. \n",
        "X_train[:,3:] = sc.fit_transform(X_train[:,3:])\n",
        "# Use only sc.transform(...) on the test dataset!\n",
        "# You need to use same Scaler used on the training dataset, otherwise you'll get a new scaler.\n",
        "# That won't make sense because X_test will be the input of the predict function\n",
        "# In order to make predictions that will be congruent with the way the model is trained,\n",
        "# we need to apply the same scaler used on the training set to the test dataset to get the same transformation\n",
        "# This is the only way to get relevant predictions.\n",
        "X_test[:,3:] = sc.transform(X_test[:,3:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)\n",
        "\n",
        "# Age and Salary are transformed\n",
        "# Now these values are on the same scale which is perfect to optimize the training of certain machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
